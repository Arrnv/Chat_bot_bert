{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UBK-yfZdwsN5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import warnings  \n",
    "with warnings.catch_warnings():  \n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HrQk0g1kwsOE"
   },
   "outputs": [],
   "source": [
    "#get the conversation and movie data\n",
    "movie_line = \"../Datasets/cornell movie-dialogs corpus/movie_lines.txt\"\n",
    "movie_convo = \"../Datasets/cornell movie-dialogs corpus/movie_conversations.txt\"\n",
    "\n",
    "m_lines = open(movie_line , encoding='utf-8',errors='ignore').read().split('\\n')\n",
    "c_lines = open(movie_convo , encoding='utf-8',errors='ignore').read().split('\\n')\n",
    "\n",
    "#get converastion lines\n",
    "convo_line = []\n",
    "for lines in c_lines:\n",
    "    _lines = lines.split(\" +++$+++ \")[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "    convo_line.append(_lines.split(\",\"))\n",
    "\n",
    "#get movie lines\n",
    "id_line = {}\n",
    "for lines in m_lines:\n",
    "    _lines = lines.split(\" +++$+++ \")\n",
    "    if len(_lines) == 5:\n",
    "        id_line[_lines[0]] = _lines[4]\n",
    "        \n",
    "#Form questions and answers \n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for line in convo_line:\n",
    "    for i in range(len(line) -1):\n",
    "        questions.append(id_line[line[i]])\n",
    "        answers.append(id_line[line[i+1]])\n",
    "        \n",
    "#Clean and replace improper words using regular expression\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"  \",\"\",text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "clean_questions = []\n",
    "clean_answers = []\n",
    "\n",
    "for q in questions:\n",
    "    clean_questions.append(clean_text(q))\n",
    "for a in answers:\n",
    "    clean_answers.append(clean_text(a))\n",
    "    \n",
    "#get the min and max length of sentence need to be used\n",
    "max_length = 5\n",
    "min_length = 2\n",
    "\n",
    "codes = ['<PAD>','<EOS>','<UNK>','<GO>']\n",
    "\n",
    "\n",
    "\n",
    "short_questions_temp = []\n",
    "short_answers_temp = []\n",
    "\n",
    "i = 0\n",
    "for question in clean_questions:\n",
    "    if len(question.split()) >= min_length and len(question.split()) <= max_length:\n",
    "        short_questions_temp.append(question)\n",
    "        short_answers_temp.append(clean_answers[i])\n",
    "    i += 1\n",
    "\n",
    "# Filter out the answers that are too short/long\n",
    "shorted_q = []\n",
    "shorted_a = []\n",
    "\n",
    "i = 0\n",
    "for answer in short_answers_temp:\n",
    "    if len(answer.split()) >= min_length and len(answer.split()) <= max_length:\n",
    "        shorted_a.append(answer)\n",
    "        shorted_q.append(short_questions_temp[i])\n",
    "    i += 1\n",
    "   \n",
    "  \n",
    "\n",
    "#Get the count of words from filtered questions and answers  \n",
    "vocab = {}\n",
    "\n",
    "for question in shorted_q:\n",
    "    for words in question.split():\n",
    "        if words not in vocab:\n",
    "            vocab[words] = 1\n",
    "        else:\n",
    "            vocab[words] +=1\n",
    "for answer in shorted_a:\n",
    "    for words in answer.split():\n",
    "        if words not in vocab:\n",
    "            vocab[words] = 1\n",
    "        else:\n",
    "            vocab[words] +=1\n",
    "            \n",
    "questions_vocabs = {}\n",
    "for answer in shorted_q:\n",
    "    for words in answer.split():\n",
    "        if words not in questions_vocabs:\n",
    "            questions_vocabs[words] = 1\n",
    "        else:\n",
    "            questions_vocabs[words] +=1\n",
    "            \n",
    "answers_vocabs = {}\n",
    "for answer in shorted_a:\n",
    "    for words in answer.split():\n",
    "        if words not in answers_vocabs:\n",
    "            answers_vocabs[words] = 1\n",
    "        else:\n",
    "            answers_vocabs[words] +=1\n",
    "            \n",
    "#total number of words appear more than 2 times\n",
    "vocabs_to_index = {}\n",
    "threshold = 2\n",
    "word_num = 0\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold:\n",
    "        vocabs_to_index[word] = word_num\n",
    "        word_num += 1\n",
    "\n",
    "#add words in codes in the text and  increment vocab index to 1 for each existing code \n",
    "#same for question and answer vocab.6281 in vocab dict and now 6286        \n",
    "for code in codes:\n",
    "    vocabs_to_index[code] = len(vocabs_to_index)+1\n",
    "    \n",
    "for code in codes:\n",
    "    questions_vocabs[code] = len(questions_vocabs)+1\n",
    "\n",
    "for code in codes:\n",
    "    answers_vocabs[code] = len(answers_vocabs)+1\n",
    "\n",
    "#Convert index vocab to vocab index   \n",
    "index_to_vocabs = {v_i: v for v, v_i in vocabs_to_index.items()}\n",
    "\n",
    "#Add <EOS> to the end of all the answer in such a way model can learn the the sentence comes to the end \n",
    "for i in range(len(shorted_a)):\n",
    "  shorted_a[i] += ' <EOS>'\n",
    "  \n",
    "#Get the question and with code <UNK> for the words which are not in vocab to index\n",
    "#ex:'nowhere hi daddy <EOS> ' to '[6285, 179, 22, 6284]' as it doesnt find the word 'nowhere' in the vocabulary index dictionary\n",
    "\n",
    "questions_int = []\n",
    "for question in shorted_q:\n",
    "    ints = []\n",
    "    for word in question.split():\n",
    "        if word not in vocabs_to_index:\n",
    "            ints.append(vocabs_to_index['<UNK>'])\n",
    "        else:\n",
    "            ints.append(vocabs_to_index[word])\n",
    "    questions_int.append(ints)\n",
    "    \n",
    "answers_int = []\n",
    "for answer in shorted_a:\n",
    "    ints = []\n",
    "    for word in answer.split():\n",
    "        if word not in vocabs_to_index:\n",
    "            ints.append(vocabs_to_index['<UNK>'])\n",
    "        else:\n",
    "            ints.append(vocabs_to_index[word])\n",
    "    answers_int.append(ints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "X0o_V2ofuDrk",
    "outputId": "6c12c65f-7cdd-4bad-f0de-b2ee693b1b63"
   },
   "outputs": [],
   "source": [
    "for code in codes:\n",
    "  print(vocabs_to_index[code])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lHBhX_UMwsOH"
   },
   "outputs": [],
   "source": [
    "target_vocab_size = len(answers_vocabs)\n",
    "source_vocab_size = len(questions_vocabs)\n",
    "vocab_size = len(index_to_vocabs)+1\n",
    "embed_size = 1024\n",
    "rnn_size = 1024\n",
    "batch_size = 32\n",
    "num_layers =  3\n",
    "learning_rate = 0.001\n",
    "learning_rate_decay = 0.99\n",
    "min_lr = 0.0001\n",
    "#keep_prob = 0.5\n",
    "epochs=50\n",
    "DISPLAY_STEP=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e2Dg_dItwsOJ"
   },
   "outputs": [],
   "source": [
    "def lstm(rnn_size, keep_prob,reuse=False):\n",
    "    lstm =tf.nn.rnn_cell.LSTMCell(rnn_size,reuse=reuse)\n",
    "    drop =tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    return drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BcGIjHaBwsON"
   },
   "outputs": [],
   "source": [
    "input_data = tf.placeholder(tf.int32, [None, None],name='input')\n",
    "target_data = tf.placeholder(tf.int32, [None, None],name='target')\n",
    "input_data_len = tf.placeholder(tf.int32,[None],name='input_len')\n",
    "target_data_len = tf.placeholder(tf.int32,[None],name='target_len')\n",
    "lr_rate = tf.placeholder(tf.float32,name='lr')\n",
    "keep_prob = tf.placeholder(tf.float32,name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "z7TNblfPwsOQ",
    "outputId": "c87b1f0d-00e1-4f90-ce76-75356864c0af"
   },
   "outputs": [],
   "source": [
    "encoder_embeddings = tf.Variable(tf.random_uniform([source_vocab_size, embed_size], -1, 1))\n",
    "encoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "Gag764m8wsOa",
    "outputId": "e792c91a-5ee8-432e-c09c-ca76978fb14c"
   },
   "outputs": [],
   "source": [
    "stacked_cells = lstm(rnn_size, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "LVt0hjBzwsOg",
    "outputId": "41ea5d68-dfc4-48c4-c010-4e13ec80e85e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "((encoder_fw_outputs,encoder_bw_outputs),\n",
    " (encoder_fw_final_state,encoder_bw_final_state)) = tf.nn.bidirectional_dynamic_rnn(cell_fw=stacked_cells, \n",
    "                                                                 cell_bw=stacked_cells, \n",
    "                                                                 inputs=encoder_embedded, \n",
    "                                                                 sequence_length=input_data_len, \n",
    "                                                                 dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QVKq1wurwsOm"
   },
   "outputs": [],
   "source": [
    "encoder_outputs = tf.concat((encoder_fw_outputs,encoder_bw_outputs),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XvdjQBE6wsOp",
    "outputId": "faa12e85-2410-461b-c558-252534f50e59"
   },
   "outputs": [],
   "source": [
    "encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QsFapqt-wsOt"
   },
   "outputs": [],
   "source": [
    "encoder_state_c = tf.concat((encoder_fw_final_state.c,encoder_bw_final_state.c),1)\n",
    "encoder_state_h = tf.concat((encoder_fw_final_state.h,encoder_bw_final_state.h),1)\n",
    "encoder_states = tf.nn.rnn_cell.LSTMStateTuple(c=encoder_state_c,h=encoder_state_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8qEgz1PdwsOv",
    "outputId": "28b5617f-64ef-4781-9df0-0e9077ac606e"
   },
   "outputs": [],
   "source": [
    "encoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ecXG9168wsOz"
   },
   "outputs": [],
   "source": [
    "main = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "decoder_input = tf.concat([tf.fill([batch_size, 1],vocabs_to_index['<GO>']), main], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UAvG9wW1wsO3"
   },
   "outputs": [],
   "source": [
    "#sam process as followed in encoder embedding and lookups\n",
    "decoder_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, embed_size], -1, 1))\n",
    "dec_cell_inputs = tf.nn.embedding_lookup(decoder_embeddings, decoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B3N5JtvZwsO8"
   },
   "outputs": [],
   "source": [
    "dec_cell = lstm(rnn_size*2,keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nfmoMirUwsO_",
    "outputId": "13b6928a-1b8b-43a4-ed8e-9331978bc2b8"
   },
   "outputs": [],
   "source": [
    "dec_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZyZQ2NR6wsPC"
   },
   "outputs": [],
   "source": [
    "#output layer for decoder\n",
    "dense_layer = tf.layers.Dense(target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "TR4iLAuDwsPJ",
    "outputId": "a9baafc5-193e-4cc2-9762-31c4174f76e0"
   },
   "outputs": [],
   "source": [
    "train_helper = tf.contrib.seq2seq.TrainingHelper(dec_cell_inputs, target_data_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5FJBKKGywsPN"
   },
   "outputs": [],
   "source": [
    "attention_cell = attention(rnn_size,encoder_outputs,target_data_len,dec_cell)\n",
    "state = attention_cell.zero_state(dtype=tf.float32, batch_size=batch_size)\n",
    "state = state.clone(cell_state=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BMXJ5P_9wsPP"
   },
   "outputs": [],
   "source": [
    "decoder_train = tf.contrib.seq2seq.BasicDecoder(cell=attention_cell, helper=train_helper, \n",
    "                                                  initial_state=state,\n",
    "                                                  output_layer=dense_layer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Vcsu2S0wsPR"
   },
   "outputs": [],
   "source": [
    "outputs_train, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder_train, \n",
    "                                                  impute_finished=True, \n",
    "                                                  maximum_iterations=tf.reduce_max(target_data_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jf02Hu8ewsPV"
   },
   "outputs": [],
   "source": [
    "infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(decoder_embeddings, \n",
    "                                                          tf.fill([batch_size], vocabs_to_index['<GO>']), \n",
    "                                                          vocabs_to_index['<EOS>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KUcM6qQOwsPX"
   },
   "outputs": [],
   "source": [
    "decoder_infer = tf.contrib.seq2seq.BasicDecoder(cell=attention_cell, helper=infer_helper, \n",
    "                                                  initial_state=state,\n",
    "                                                  output_layer=dense_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pq0kpF-BwsPY"
   },
   "outputs": [],
   "source": [
    "outputs_infer, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder_infer, impute_finished=True,\n",
    "                                                          maximum_iterations=tf.reduce_max(target_data_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D7e00xllwsPb"
   },
   "outputs": [],
   "source": [
    "training_logits = tf.identity(outputs_train.rnn_output, name='logits')\n",
    "inference_logits = tf.identity(outputs_infer.sample_id, name='predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCczbxr-wsPd"
   },
   "outputs": [],
   "source": [
    "masks = tf.sequence_mask(target_data_len, tf.reduce_max(target_data_len), dtype=tf.float32, name='masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "txKobnLEwsPf"
   },
   "outputs": [],
   "source": [
    "cost = tf.contrib.seq2seq.sequence_loss(training_logits,target_data,masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wsg4xpVIwsPi"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(lr_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jcrBNbOFwsPk"
   },
   "outputs": [],
   "source": [
    "gradients = optimizer.compute_gradients(cost)\n",
    "capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hy8u3-bjwsPo"
   },
   "outputs": [],
   "source": [
    "def pad_sentence(sentence_batch, pad_int):\n",
    "    padded_seqs = []\n",
    "    seq_lens = []\n",
    "    max_sentence_len = max([len(sentence) for sentence in sentence_batch])\n",
    "    for sentence in sentence_batch:\n",
    "        padded_seqs.append(sentence + [pad_int] * (max_sentence_len - len(sentence)))\n",
    "        seq_lens.append(len(sentence))\n",
    "    return padded_seqs, seq_lens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SgVkMCsmwsPq"
   },
   "outputs": [],
   "source": [
    "def get_accuracy(target, logits):\n",
    "    max_seq = max(len(target[1]), logits.shape[1])\n",
    "    if max_seq - len(target[1]):\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - len(target[1]))],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V8oeeDm-wsPr"
   },
   "outputs": [],
   "source": [
    "train_data = questions_int[batch_size:]\n",
    "test_data = answers_int[batch_size:]\n",
    "val_train_data = questions_int[:batch_size]\n",
    "val_test_data = answers_int[:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MQ-XF1liwsPu",
    "outputId": "40994dc8-d3b9-4e62-ba9c-3a05e922628a"
   },
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tsSesPuRwsPw"
   },
   "outputs": [],
   "source": [
    "pad_int = vocabs_to_index['<PAD>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9-KGHuRrwsPz"
   },
   "outputs": [],
   "source": [
    "val_batch_x,val_batch_len = pad_sentence(val_train_data,pad_int)\n",
    "val_batch_y,val_batch_len_y = pad_sentence(val_test_data,pad_int)\n",
    "val_batch_x = np.array(val_batch_x)\n",
    "val_batch_y = np.array(val_batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CzX0zA7zwsP2"
   },
   "outputs": [],
   "source": [
    "no_of_batches = math.floor(len(train_data)//batch_size)\n",
    "round_no = no_of_batches*batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_seq(sentence, vocabs_to_index):\n",
    "    results = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        if word in vocabs_to_index:\n",
    "            results.append(vocabs_to_index[word])\n",
    "        else:\n",
    "            results.append(vocabs_to_index['<UNK>'])        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_sentence = 'where are you'\n",
    "question_sentence = sentence_to_seq(question_sentence, vocabs_to_index)\n",
    "print(question_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_writer = tf.summary.FileWriter('D:/ML Projects/Global IA/Seq2Seq-Chatbot/Notebook/model_weights/log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summaries_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51017
    },
    "colab_type": "code",
    "id": "3T3wGVUEwsP5",
    "outputId": "aa9a1f50-72eb-4fa1-b1f5-cddfec4376e0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "save_path = '/ML Projects/Global IA/Seq2Seq-Chatbot/Notebook/model_weights/model_weights'\n",
    "acc_plt = []\n",
    "loss_plt = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(epochs):\n",
    "        #_, summaries_str = sess.run([train_op, summaries_op])\n",
    "        #fw.add_summary(summaries_str, global_step=i)\n",
    "        total_accuracy = 0.0\n",
    "        total_loss = 0.0\n",
    "        for bs in tqdm(range(0,round_no  ,batch_size)):\n",
    "          index = min(bs+batch_size, round_no )\n",
    "          #print(bs,index)\n",
    "      \n",
    "          #padding done seperately for each batch in training and testing data\n",
    "          batch_x,len_x = pad_sentence(train_data[bs:index],pad_int)\n",
    "          batch_y,len_y = pad_sentence(test_data[bs:index],pad_int)\n",
    "          batch_x = np.array(batch_x)\n",
    "          batch_y = np.array(batch_y)\n",
    "        \n",
    "          pred,loss_f,opt = sess.run([inference_logits,cost,train_op], \n",
    "                                      feed_dict={input_data:batch_x,\n",
    "                                                target_data:batch_y,\n",
    "                                                input_data_len:len_x,\n",
    "                                                target_data_len:len_y,\n",
    "                                                lr_rate:learning_rate,\n",
    "                                                keep_prob:0.75})\n",
    "\n",
    "          train_acc = get_accuracy(batch_y, pred)\n",
    "          total_loss += loss_f \n",
    "          total_accuracy+=train_acc\n",
    "    \n",
    "        total_accuracy /= (round_no // batch_size)\n",
    "    \n",
    "        total_loss /=  (round_no//batch_size)\n",
    "        acc_plt.append(total_accuracy)\n",
    "        loss_plt.append(total_loss)\n",
    "        prediction_logits = sess.run(inference_logits, {input_data: [question_sentence]*batch_size,\n",
    "                                         input_data_len: [len(question_sentence)]*batch_size,\n",
    "                                         target_data_len: [len(question_sentence)]*batch_size,              \n",
    "                                         keep_prob: 0.75,\n",
    "                                         })[0]\n",
    "        print('Epoch %d,Average_loss %f, Average Accucracy %f'%(epoch+1,total_loss,total_accuracy))\n",
    "        print('  Inputs Words: {}'.format([index_to_vocabs[i] for i in question_sentence]))\n",
    "        print('  Replied Words: {}'.format(\" \".join([index_to_vocabs[i] for i in prediction_logits])))\n",
    "        print('\\n')\n",
    "        saver = tf.train.Saver() \n",
    "        saver.save(sess, save_path)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir /ML Projects/Global IA/Seq2Seq-Chatbot/Notebook/model_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "WkzXCp_swsP9",
    "outputId": "1ba662bb-8c48-4349-bb68-2635ccb58412"
   },
   "outputs": [],
   "source": [
    "#Accuracy vs Epochs\n",
    "plt.plot(range(epochs),acc_plt)\n",
    "plt.title(\"Change in Accuracy\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "HcbTo1j0y9my",
    "outputId": "de08d92a-2dfe-4c31-fcaf-f186572108c7"
   },
   "outputs": [],
   "source": [
    "#loss vs Epochs\n",
    "plt.plot(range(epochs),loss_plt)\n",
    "plt.title(\"Change in loss\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Lost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VwUlywN4Ajzl"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "39MSfl6AAlTS"
   },
   "outputs": [],
   "source": [
    "pickle.dump(acc_plt,open('accuracy.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XuRLcdV-BJQF"
   },
   "outputs": [],
   "source": [
    "pickle.dump(loss_plt,open('loss.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "Z9tjyWtGtorR",
    "outputId": "43d99274-29ef-4842-a837-ccac6f6bdc80"
   },
   "outputs": [],
   "source": [
    "#get all the codes/tokens we additionaly added in the vocab dictionary\n",
    "garbage = []\n",
    "for code in codes:\n",
    "  print(vocabs_to_index[code])\n",
    "  garbage.append(vocabs_to_index[code])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5kv4x-p94Rvk"
   },
   "outputs": [],
   "source": [
    "#prepare the question,answer and prediction data\n",
    "def print_data(i,batch_x,index_to_vocabs):\n",
    "  data = []\n",
    "  for n in batch_x[i]:\n",
    "    if n==garbage[1]:\n",
    "      break\n",
    "    else:\n",
    "      if n not in [6283,6285,6286]:\n",
    "        data.append(index_to_vocabs[n])\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bC1ih2B396Bm"
   },
   "outputs": [],
   "source": [
    "ques = []\n",
    "real_answer = []\n",
    "pred_answer = []\n",
    "for i in range(len(val_batch_x)):\n",
    "  ques.append(print_data(i,batch_x,index_to_vocabs))\n",
    "  real_answer.append(print_data(i,batch_y,index_to_vocabs))\n",
    "  pred_answer.append(print_data(i,pred,index_to_vocabs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 10897
    },
    "colab_type": "code",
    "id": "QvUS1zq3_zqL",
    "outputId": "2c12f491-01cd-4486-b0af-00993da6780d"
   },
   "outputs": [],
   "source": [
    "for i in range(len(val_batch_x)):\n",
    "    print('row %d'%(i+1))\n",
    "    print('QUESTION:',' '.join(ques[i]))\n",
    "    print('REAL ANSWER:',' '.join(real_answer[i]))\n",
    "    print('PREDICTED ANSWER:',' '.join(pred_answer[i]),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "30qfB77zEMx8",
    "outputId": "457a6348-e21c-4d52-efe6-74143ec48e59"
   },
   "outputs": [],
   "source": [
    "question_sentence_2 = 'what are you doing?'\n",
    "question_sentence_2 = sentence_to_seq(question_sentence_2, vocabs_to_index)\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(save_path + '.meta')\n",
    "    loader.restore(sess, save_path)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    input_data_len = loaded_graph.get_tensor_by_name('input_len:0')\n",
    "    target_data_len = loaded_graph.get_tensor_by_name('target_len:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "    prediction_logits = sess.run(logits, {input_data: [question_sentence_2]*batch_size,\n",
    "                                         input_data_len: [len(question_sentence_2)]*batch_size,\n",
    "                                         target_data_len : [5]*batch_size,\n",
    "                                         keep_prob: 1.0})[0]\n",
    "\n",
    "print('Input')\n",
    "print('  Word Ids:      {}'.format([i for i in question_sentence_2]))\n",
    "print('  Question: {}'.format([index_to_vocabs[i] for i in question_sentence_2]))\n",
    "\n",
    "print('\\nPrediction')\n",
    "print('  Word Ids:      {}'.format([i for i in prediction_logits]))\n",
    "print('  Answer: {}'.format(\" \".join([index_to_vocabs[i] for i in prediction_logits])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter('D:/ML Projects/Global IA/Seq2Seq-Chatbot/Notebook/model_weights/log', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "encoder = info.features['text'].encoder\n",
    "\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for num, word in enumerate(encoder.subwords):\n",
    "  vec = weights[num+1] # skip 0, it's padding.\n",
    "  out_m.write(word + \"\\n\")\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "new_chatbot_checkling.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
